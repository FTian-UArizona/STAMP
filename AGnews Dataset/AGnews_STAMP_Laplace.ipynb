{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyOt/INiv5b8vOX3TJ+GwXHU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# --- Load My Functions ---\n","# Put the file name functions.py under folder\n","import functions\n","from functions import *\n","\n","import GPT_function\n","from GPT_function import *"],"metadata":{"id":"M9hQyGBFlV0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"opy49YOY1Pqy","executionInfo":{"status":"aborted","timestamp":1763766705468,"user_tz":420,"elapsed":198,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"outputs":[],"source":["# Library\n","\n","import torch\n","import math\n","import re\n","import numpy as np\n","import pandas as pd\n","from torch.distributions import Laplace\n","\n","from transformers import AutoTokenizer\n","from transformers import AutoModel\n","from transformers import AutoModelForCausalLM\n","from transformers import GPT2LMHeadModel\n","\n","from datasets import load_dataset\n","from sentence_transformers import SentenceTransformer, util\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from scipy.stats import vonmises_fisher\n","import torch.nn.functional as F\n","\n","from typing import Dict, List, Optional\n","\n","from openai import OpenAI\n","\n","from collections import Counter"]},{"cell_type":"code","source":["# Change this if possible\n","# Also change the one in GPT_function\n","\n","client = OpenAI(api_key=\"Your_API_Key\")  # needs OPENAI_API_KEY\n"],"metadata":{"id":"37Qdf9kEIG6l","executionInfo":{"status":"aborted","timestamp":1763766705494,"user_tz":420,"elapsed":223,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load only the first 50 examples of the test split of the Yelp dataset\n","ds = load_dataset(\"ag_news\", split=\"test[:50]\")\n"],"metadata":{"id":"usDoO5Y9Sezw","executionInfo":{"status":"aborted","timestamp":1763766705496,"user_tz":420,"elapsed":0,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Load tokenizer and GPT-2 model ---\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\", output_hidden_states=True)\n","embedding_table = model.get_input_embeddings().weight.detach()\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","model.eval()\n","\n","# Base tokenizer pad fix (optional)\n","if getattr(tokenizer, \"pad_token_id\", None) is None and getattr(tokenizer, \"eos_token\", None) is not None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","# Load a light GPT-2 model\n","gpt2_tok = AutoTokenizer.from_pretrained(\"distilgpt2\")\n","if gpt2_tok.pad_token is None:\n","    gpt2_tok.pad_token = gpt2_tok.eos_token\n","gpt2_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(\n","    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",").eval()\n","\n","# --- Extract embedding table ---\n","# Normalize embedding table for search\n","norm_embedding_table = torch.nn.functional.normalize(embedding_table, dim=1)"],"metadata":{"id":"YZecP5w2lVxq","executionInfo":{"status":"aborted","timestamp":1763766705498,"user_tz":420,"elapsed":1,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sOYdjIRM06O9","executionInfo":{"status":"aborted","timestamp":1763766705499,"user_tz":420,"elapsed":221,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Heuristic privacy flags at the word level (_privacy_flag_for_word)\n","# A word is marked private = True if any of these fire:\n","\n","# 1. Looks like email or URL ([A-Z0-9._%+-]+@..., http(s):// or www.).\n","\n","# 2. Contains a digit (covers dates, addresses, ages, years, etc.).\n","\n","# 3. Alphanumeric mix like A12B or user123.\n","\n","# 4. Is a month name/abbr. (e.g., jan, feb, september, …).\n","\n","# 5. Ends with common location suffixes (e.g., -ville, -town, -city, -grad, …).\n","\n","# 6. Proper-noun-ish capitalization or CamelCase chunk.\n","\n","# 7. Starts with @ or # (handles/hashtags).\n","\n","# 8. Long ID-ish tokens (≥6) with underscores/hyphens allowed.\n","\n","# Every subtoken of that word gets the same private value."],"metadata":{"id":"qVMqefXL06UK","executionInfo":{"status":"aborted","timestamp":1763766705500,"user_tz":420,"elapsed":222,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Important? (token-level cosine to the question)\n","\n","# Question centroid\n","\n","# 1. Take the (static) GPT-2 embedding for each question token, L2-normalize each, then average and re-normalize → q.\n","\n","# Token–question similarity\n","\n","# 2. For each context token, take its (static) embedding, L2-normalize, compute cos = dot(token_emb, q).\n","\n","# Threshold by τ, important = (cos ≥ τ), with τ set by --tau (default 0.5).\n","\n","# Note: this is not a contextual encoder; it’s using the GPT-2 embedding table only, so it’s a simple, fast proxy for task relevance."],"metadata":{"id":"Mr1I-2TL1Nql","executionInfo":{"status":"aborted","timestamp":1763766705500,"user_tz":420,"elapsed":222,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This is actual STAMP\n","# This is a play around parameter for importance , default 0.5\n","tau = 0.50\n","\n","@torch.no_grad()\n","def get_task_vector(question: str, tokenizer, embedding_table: torch.Tensor) -> torch.Tensor:\n","    q_ids = tokenizer.encode(question, add_special_tokens=False)\n","    if len(q_ids) == 0:\n","        v = torch.randn(embedding_table.shape[1], device=embedding_table.device)\n","        return F.normalize(v, dim=0)\n","    q_ids_t = torch.tensor(q_ids, dtype=torch.long, device=embedding_table.device)\n","    q_vec = embedding_table[q_ids_t].mean(dim=0)\n","    return F.normalize(q_vec, dim=0)\n","\n","# ---- Light NER-ish fallback (unchanged, just kept here)\n","_CAP  = re.compile(r\"^[A-Z][a-z]{2,}$\")\n","_EMAIL= re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n","_NUM  = re.compile(r\"^\\d{2,}$\")\n","def _clean_piece(p: str) -> str:\n","    return p.lstrip(\"Ġ▁\").lstrip(\"##\")\n","def fallback_ner(tokens):\n","    toks = [_clean_piece(t) for t in tokens]\n","    return [bool(_EMAIL.search(t) or _NUM.match(t) or _CAP.match(t)) for t in toks]\n","\n","# ==== canonical partitioner ====\n","@torch.no_grad()\n","def partition_tokens_2x2(context: str,\n","                         question: str,\n","                         tokenizer,\n","                         embedding_table: torch.Tensor,\n","                         tau: float = tau,\n","                         ner_fn=None):\n","    \"\"\"\n","    Returns: token_ids (List[int]), tokens (List[str]), groups (List[int] in {1,2,3,4})\n","      1 = High-privacy × High-importance\n","      2 = High-privacy × Low-importance\n","      3 = Low-privacy  × High-importance\n","      4 = Low-privacy  × Low-importance\n","    \"\"\"\n","    token_ids = tokenizer.encode(context, add_special_tokens=False)\n","    pieces = tokenizer.convert_ids_to_tokens(token_ids)\n","\n","    # Importance via cosine to task vector\n","    q_vec = get_task_vector(question, tokenizer, embedding_table)     # (d,)\n","    ids_t = torch.tensor(token_ids, dtype=torch.long, device=embedding_table.device)\n","    vecs  = embedding_table[ids_t]                                    # [n,d]\n","    vecs  = F.normalize(vecs, dim=1)\n","    sims  = (vecs @ q_vec)                                            # [n]\n","    high_imp = sims.ge(tau).tolist()\n","\n","    # Privacy via NER-ish heuristic by default\n","    high_priv = (ner_fn or fallback_ner)(pieces)\n","\n","    groups = []\n","    for p, imp in zip(high_priv, high_imp):\n","        if p and imp:         groups.append(1)\n","        elif p and not imp:   groups.append(2)\n","        elif (not p) and imp: groups.append(3)\n","        else:                 groups.append(4)\n","\n","    # Counts per group\n","    cnt = Counter(groups)\n","    total = len(groups)\n","    print(f\"τ={tau}  |  tokens={total}  |  G1={cnt.get(1,0)}  G2={cnt.get(2,0)}  G3={cnt.get(3,0)}  G4={cnt.get(4,0)}\")\n","\n","    return token_ids, pieces, groups\n","\n","\n","# ---- Back-compat alias so existing calls don't break (IMPORTANT):\n","partition_tokens_paper = partition_tokens_2x2\n","\n","# ---- Helper that returns ONLY the groups (for apply_stamp callers)\n","def groups_2x2(context: str,\n","               question: str,\n","               tokenizer,\n","               embedding_table: torch.Tensor,\n","               **kw) -> list[int]:\n","    tok_ids, _, groups = partition_tokens_2x2(context, question, tokenizer, embedding_table, **kw)\n","    return groups, tok_ids"],"metadata":{"id":"DGzP0TsQGj6x","executionInfo":{"status":"aborted","timestamp":1763766705502,"user_tz":420,"elapsed":223,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JxxpGh320pDB","executionInfo":{"status":"aborted","timestamp":1763766705502,"user_tz":420,"elapsed":223,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Unit test on partition"],"metadata":{"id":"P6-8Q7LwK4Sy","executionInfo":{"status":"aborted","timestamp":1763766705503,"user_tz":420,"elapsed":224,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Print tokens by 2×2 group (uses current STAMP.ipynb code) ----\n","def print_tokens_by_group(\n","    context: str,\n","    question: str,\n","    tau: float = None,\n","    *,\n","    labels: dict | None = None,\n","    show_counts: bool = True,\n","    return_buckets: bool = False,\n","):\n","    \"\"\"\n","    Prints the BPE tokens grouped into:\n","      G1: important + private\n","      G2: !important + private\n","      G3: important + !private\n","      G4: !important + !private\n","\n","    Uses the notebook's partitioning function:\n","      - partition_tokens_2x2(context, question, tokenizer, embedding_table, tau=...)\n","        -> (token_ids, pieces, groups)\n","    Falls back to:\n","      - groups_2x2(...) -> (groups, token_ids) + tokenizer.convert_ids_to_tokens(...)\n","    \"\"\"\n","    # default to the notebook's global tau if not provided\n","    if tau is None:\n","        try:\n","            _ = tau  # local arg\n","        except NameError:\n","            pass\n","        tau = globals().get(\"tau\", 0.50)\n","\n","    # Try the main API: partition_tokens_2x2 returns pieces directly\n","    pieces = None\n","    try:\n","        token_ids, pieces, groups = partition_tokens_2x2(\n","            context=context,\n","            question=question,\n","            tokenizer=tokenizer,\n","            embedding_table=embedding_table,\n","            tau=tau,\n","        )\n","    except NameError:\n","        # Back-compat path if only groups_2x2 is defined\n","        groups, token_ids = groups_2x2(\n","            context=context,\n","            question=question,\n","            tokenizer=tokenizer,\n","            embedding_table=embedding_table,\n","            tau=tau,\n","        )\n","    # If pieces not provided, derive from ids\n","    if pieces is None:\n","        pieces = tokenizer.convert_ids_to_tokens(token_ids)\n","\n","    # Bucket tokens\n","    buckets = {1: [], 2: [], 3: [], 4: []}\n","    for tok, g in zip(pieces, groups):\n","        buckets[g].append(tok)\n","\n","    lab = labels or {\n","        1: \"G1 (imp+priv)\",\n","        2: \"G2 (!imp+priv)\",\n","        3: \"G3 (imp+!priv)\",\n","        4: \"G4 (!imp+!priv)\",\n","    }\n","\n","    for g in (1, 2, 3, 4):\n","        header = f\"\\n{lab[g]}\"\n","        if show_counts:\n","            header += f\" — {len(buckets[g])} tokens\"\n","        print(header + \":\")\n","        print(\" \".join(buckets[g]) if buckets[g] else \"—\")\n","\n","    if return_buckets:\n","        return buckets\n"],"metadata":{"id":"ShL8fLJT0pLi","executionInfo":{"status":"aborted","timestamp":1763766705504,"user_tz":420,"elapsed":224,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate sweep maps\n","def generate_eps_maps_with_step(start_map, end_map, step):\n","    \"\"\"Generates a list of eps_map dictionaries with values incremented by step.\"\"\"\n","    maps = []\n","    current_map = start_map.copy()\n","    while all(current_map.get(k, float('inf')) <= end_map.get(k, float('inf')) for k in end_map):\n","        maps.append(current_map.copy())\n","        next_map = {}\n","        for key in end_map:\n","            start_val = start_map.get(key, float('inf'))\n","            end_val = end_map.get(key, float('inf'))\n","            current_val = current_map.get(key, float('inf'))\n","            if current_val == float('inf'):\n","                 next_map[key] = float('inf')\n","            else:\n","                next_map[key] = current_val + step[key] if key in step else current_val + step.get('default', 0)\n","                if next_map[key] > end_val and end_val != float('inf'):\n","                    next_map[key] = end_val\n","        current_map = next_map\n","        # Break if no values were incremented (handles cases where start == end or step is 0)\n","        if current_map == maps[-1]:\n","            break\n","\n","    # Ensure the end_map is included if it wasn't reached exactly\n","    if maps and maps[-1] != end_map:\n","        maps.append(end_map.copy())\n","\n","    return maps\n"],"metadata":{"id":"l-Yzvy7FUzwg","executionInfo":{"status":"ok","timestamp":1763766705507,"user_tz":420,"elapsed":2,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Lets try an example\n","ctx = \"Barack Obama was born in Hawaii on August 4, 1961.\"\n","q   = \"Where was Obama born?\"\n","# Note: The importance calculation in print_tokens_by_group is based on the relationship with a question.\n","# For the Yelp review dataset, which does not have inherent questions, this aspect might not be directly applicable.\n","# We will still call the function, but keep in mind the 'importance' groups are relative to this example question.\n","print_tokens_by_group(ctx, q, tau=0.50)\n","\n","ds = load_dataset(\"ag_news\", split=\"test[:1]\")\n","\n","for ex in ds:\n","    # Access the review text using the 'text' key\n","    ctx = ex[\"text\"]\n","    # Access the rating using the 'label' key (as seen from the dataset structure)\n","    rating = ex[\"label\"]\n","\n","    print(f\"\\nReview Text: {ctx}\")\n","    print(f\"Rating: {rating} stars\")\n","\n","    # Since the Yelp dataset does not have inherent questions, we will use a generic\n","    # approach for partitioning. The 'importance' aspect as calculated by\n","    # print_tokens_by_group (based on a question) may not be meaningful here.\n","    # We will still call the function for demonstration purposes, but be aware\n","    # of how importance is being determined in this context.\n","    # Using a generic question, or could consider removing importance calculation\n","    # if not relevant for the task with this dataset.\n","    q = \"Which category does this news article belong to: World, Sports, Business, or Sci/Tech?\"\n","\n","    # Use the canonical partitioner\n","    print_tokens_by_group(ctx, q, tau=0.50)"],"metadata":{"id":"BLCLGLun0pN7","executionInfo":{"status":"error","timestamp":1763766705517,"user_tz":420,"elapsed":9,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}},"collapsed":true,"colab":{"base_uri":"https://localhost:8080/","height":211},"outputId":"b2fd6370-4ce3-4e8b-bcf5-3d66b75b87c5"},"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'print_tokens_by_group' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-357682431.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# For the Yelp review dataset, which does not have inherent questions, this aspect might not be directly applicable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# We will still call the function, but keep in mind the 'importance' groups are relative to this example question.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint_tokens_by_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ag_news\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test[:1]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'print_tokens_by_group' is not defined"]}]},{"cell_type":"code","source":["# Unit test on Squad"],"metadata":{"id":"aZopyhJM3MCn","executionInfo":{"status":"aborted","timestamp":1763766705623,"user_tz":420,"elapsed":25,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rlyStZ2DSvD4","executionInfo":{"status":"aborted","timestamp":1763766705625,"user_tz":420,"elapsed":1,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"da77c72c","executionInfo":{"status":"aborted","timestamp":1763766705629,"user_tz":420,"elapsed":344,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}},"collapsed":true},"source":["# Apply apply_stamp_laplace\n","results_laplace = []\n","\n","ds = load_dataset(\"ag_news\", split=\"test[:50]\")\n","eps_map = {1:1650, 2:1550, 3:1950, 4:1850}\n","# {\\{1650, 1550, 1950, 1850\\}}\n","for ex in ds:\n","    ctx = ex[\"text\"]\n","    q = \"Which category does this news article belong to: World, Sports, Business, or Sci/Tech?\"\n","\n","    answers = ex[\"label\"] # Extract answers\n","\n","    # Use the canonical partitioner\n","    tok_ids, _, groups = partition_tokens_2x2(\n","        context=ctx,\n","        question=q,\n","        tokenizer=tokenizer,\n","        embedding_table=embedding_table,\n","        tau=0.50,                 # or tune\n","        # ner_fn=your_spacy_ner   # optional: plug a heavier NER here\n","    )\n","\n","    privatized_ctx_laplace = apply_stamp_laplace(\n","        torch.tensor(tok_ids, device=embedding_table.device),\n","        groups,\n","        embedding_table,\n","        tokenizer,\n","        eps_dir_by_group=eps_map,                       # <-- actually used\n","    )[0]\n","\n","    results_laplace.append({\n","        \"question\": q,\n","        \"rating\": answers,\n","        \"original_context\": ctx,\n","        \"privatized_context\": privatized_ctx_laplace,\n","    })\n","\n","# Create a pandas DataFrame and save it to CSV\n","df_results_laplace = pd.DataFrame(results_laplace)\n","df_results_laplace.to_csv(f\"AGnews_stamp_Laplace_tau_{tau:.1f}.csv\", index=False)\n","\n","print(\"Processing complete. Results saved to AGnews_stamp_laplace_tau.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply apply_stamp_laplace for different privacy maps\n","\n","start_eps_map = {1: 150, 2: 50, 3: 450, 4: 350}\n","end_eps_map = {1: 350, 2: 250, 3: 650, 4: 550}\n","step_eps_map = {1: 50, 2: 50, 3: 50, 4: 50} # Step size for each key\n","\n","eps_maps_sweep = generate_eps_maps_with_step(start_eps_map, end_eps_map, step_eps_map)\n","\n","ds = load_dataset(\"ag_news\", split=\"test[:50]\")\n","\n","for i, eps_map in enumerate(eps_maps_sweep):\n","    print(f\"Processing with eps_map = {eps_map}\")\n","    results_laplace = []\n","    total_epsilon_sum = 0\n","    total_tokens_count = 0\n","\n","    for ex in ds:\n","        # Access the review text using the 'text' key\n","        ctx = ex[\"text\"]\n","        # Use a placeholder question since this is a review dataset\n","        q = \"Which category does this news article belong to: World, Sports, Business, or Sci/Tech?\"\n","\n","        # Access the rating using the 'label' key\n","        rating = ex[\"label\"]\n","\n","        # Use the canonical partitioner\n","        tok_ids, _, groups = partition_tokens_2x2(\n","            context=ctx,\n","            question=q,\n","            tokenizer=tokenizer,\n","            embedding_table=embedding_table,\n","            tau=0.50,  # Using a fixed tau for this sweep\n","        )\n","\n","        # Calculate total epsilon and token count for average calculation\n","        sentence_epsilon_sum = sum(eps_map[group] for group in groups)\n","        total_epsilon_sum += sentence_epsilon_sum\n","        total_tokens_count += len(tok_ids)\n","\n","\n","        privatized_ctx_laplace = apply_stamp_laplace(\n","            torch.tensor(tok_ids, device=embedding_table.device),\n","            groups,\n","            embedding_table,\n","            tokenizer,\n","            eps_dir_by_group=eps_map,  # <-- actually used\n","        )[0]\n","\n","        # Removed: Apply repair_coherent_gpt4\n","        # repaired_ctx_laplace = repair_coherent_gpt4(privatized_ctx_laplace, answer_span=None)\n","\n","        results_laplace.append(\n","            {\n","                \"question\": q,\n","                \"rating\": rating, # Store rating instead of answers\n","                \"original_context\": ctx,\n","                \"privatized_context\": privatized_ctx_laplace,\n","                # Removed: \"repaired_context\": repaired_ctx_laplace,\n","            }\n","        )\n","\n","    # Calculate average epsilon per token for this eps_map\n","    average_epsilon_per_token = total_epsilon_sum / total_tokens_count if total_tokens_count > 0 else 0\n","\n","    # Create a pandas DataFrame and save it to CSV with average epsilon in the filename\n","    # Using a slightly different filename to distinguish from SQuAD results\n","    df_results_laplace = pd.DataFrame(results_laplace)\n","    df_results_laplace.to_csv(f\"AGnews_stamp_Laplace_sweep_avg_epsilon_{average_epsilon_per_token:.2f}.csv\", index=False)\n","\n","    print(f\"Processing complete for eps_map {i+1}. Results saved to AGnews_stamp_Laplace_sweep_avg_epsilon_{average_epsilon_per_token:.2f}.csv\")\n","\n","print(\"All sweeps complete.\")"],"metadata":{"id":"FNuQtz10EWnb","collapsed":true,"executionInfo":{"status":"aborted","timestamp":1763766705630,"user_tz":420,"elapsed":343,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yK0C-8Nl4Yjz","executionInfo":{"status":"aborted","timestamp":1763766705631,"user_tz":420,"elapsed":344,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Add a special case for eps_map = {1: 100, 2: 0, 3: 350, 4: 250} using Laplace\n","\n","special_eps_map_laplace = {1: 100, 2: 1, 3: 350, 4: 250}\n","\n","ds = load_dataset(\"ag_news\", split=\"test[:50]\")\n","\n","print(f\"Processing with special_eps_map_laplace = {special_eps_map_laplace}\")\n","results_laplace = []\n","total_epsilon_sum = 0\n","total_tokens_count = 0\n","\n","for ex in ds:\n","    ctx = ex[\"text\"]\n","    q = \"Which category does this news article belong to: World, Sports, Business, or Sci/Tech?\"\n","\n","    rating = ex[\"label\"]\n","\n","    tok_ids, _, groups = partition_tokens_2x2(\n","        context=ctx,\n","        question=q,\n","        tokenizer=tokenizer,\n","        embedding_table=embedding_table,\n","        tau=0.50,\n","    )\n","\n","    sentence_epsilon_sum = sum(special_eps_map_laplace[group] for group in groups)\n","    total_epsilon_sum += sentence_epsilon_sum\n","    total_tokens_count += len(tok_ids)\n","\n","    privatized_ctx_laplace = apply_stamp_laplace(\n","        torch.tensor(tok_ids, device=embedding_table.device),\n","        groups,\n","        embedding_table,\n","        tokenizer,\n","        eps_dir_by_group=special_eps_map_laplace,\n","    )[0]\n","\n","    results_laplace.append(\n","        {\n","            \"question\": q,\n","            \"rating\": rating,\n","            \"original_context\": ctx,\n","            \"privatized_context\": privatized_ctx_laplace,\n","        }\n","    )\n","\n","average_epsilon_per_token = total_epsilon_sum / total_tokens_count if total_tokens_count > 0 else 0\n","\n","df_results_laplace = pd.DataFrame(results_laplace)\n","df_results_laplace.to_csv(f\"AGnews_stamp_laplace_special_avg_epsilon_{average_epsilon_per_token:.2f}.csv\", index=False)\n","\n","print(f\"Processing complete for special_eps_map_laplace. Results saved to AGnews_stamp_laplace_special_avg_epsilon_{average_epsilon_per_token:.2f}.csv\")"],"metadata":{"id":"A5BlPj9zEWqP","executionInfo":{"status":"aborted","timestamp":1763766705632,"user_tz":420,"elapsed":344,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eMq1Ibix4u6J","executionInfo":{"status":"aborted","timestamp":1763766705633,"user_tz":420,"elapsed":345,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply apply_stamp_laplace after apply_stamp_laplace for different privacy maps\n","\n","def generate_eps_maps_with_step(start_map, end_map, step):\n","    \"\"\"Generates a list of eps_map dictionaries with values incremented by step.\"\"\"\n","    maps = []\n","    current_map = start_map.copy()\n","    while all(current_map.get(k, float('inf')) <= end_map.get(k, float('inf')) for k in end_map):\n","        maps.append(current_map.copy())\n","        next_map = {}\n","        for key in end_map:\n","            start_val = start_map.get(key, float('inf'))\n","            end_val = end_map.get(key, float('inf'))\n","            current_val = current_map.get(key, float('inf'))\n","            if current_val == float('inf'):\n","                 next_map[key] = float('inf')\n","            else:\n","                next_map[key] = current_val + step[key] if key in step else current_val + step.get('default', 0)\n","                if next_map[key] > end_val and end_val != float('inf'):\n","                    next_map[key] = end_val\n","        current_map = next_map\n","        # Break if no values were incremented (handles cases where start == end or step is 0)\n","        if current_map == maps[-1]:\n","            break\n","\n","    # Ensure the end_map is included if it wasn't reached exactly\n","    if maps and maps[-1] != end_map:\n","        maps.append(end_map.copy())\n","\n","    return maps\n","\n","start_eps_map = {1: 150, 2: 50, 3: 450, 4: 350}\n","end_eps_map = {1: 350, 2: 250, 3: 650, 4: 550}\n","step_eps_map = {1: 50, 2: 50, 3: 50, 4: 50} # Step size for each key\n","\n","eps_maps_sweep = generate_eps_maps_with_step(start_eps_map, end_eps_map, step_eps_map)\n","\n","ds = load_dataset(\"ag_news\", split=\"test[:50]\")\n","\n","for i, eps_map in enumerate(eps_maps_sweep):\n","    print(f\"Processing with eps_map = {eps_map}\")\n","    results_laplace = []\n","    total_epsilon_sum = 0\n","    total_tokens_count = 0\n","\n","    for ex in ds:\n","        # Access review text and rating using correct keys for Yelp dataset\n","        ctx = ex[\"text\"]\n","        q = \"Which category does this news article belong to: World, Sports, Business, or Sci/Tech?\"\n","\n","        rating = ex[\"label\"]  # Access rating\n","\n","        # Use the canonical partitioner\n","        tok_ids, _, groups = partition_tokens_2x2(\n","            context=ctx,\n","            question=q,\n","            tokenizer=tokenizer,\n","            embedding_table=embedding_table,\n","            tau=0.50,  # Using a fixed tau for this sweep\n","        )\n","\n","        # Calculate total epsilon and token count for average calculation\n","        sentence_epsilon_sum = sum(eps_map[group] for group in groups)\n","        total_epsilon_sum += sentence_epsilon_sum\n","        total_tokens_count += len(tok_ids)\n","\n","        # Calculate and print the pre-token average budget for each sentence\n","        pre_token_average_budget = sentence_epsilon_sum / len(tok_ids) if len(tok_ids) > 0 else 0\n","        print(f\"Pre-token average budget for this sentence: {pre_token_average_budget:.2f}\")\n","\n","        # Create a new eps_map for uniform budget for this sentence\n","        # Note: This logic seems to be applying the average sentence budget uniformly across groups,\n","        # which might not be the intended behavior if you want to maintain differential privacy across groups.\n","        # Consider if you want to apply the original eps_map or this uniform one.\n","        uniform_eps_map = {1: pre_token_average_budget, 2: pre_token_average_budget, 3: pre_token_average_budget, 4: pre_token_average_budget}\n","\n","\n","        privatized_ctx_laplace = apply_stamp_laplace(\n","            torch.tensor(tok_ids, device=embedding_table.device),\n","            groups,\n","            embedding_table,\n","            tokenizer,\n","            eps_dir_by_group=uniform_eps_map,  # <-- actually used (using uniform map)\n","        )[0]\n","\n","        # Removed: Apply repair_coherent_gpt4 as it's designed for QA pairs\n","        # repaired_ctx_laplace = repair_coherent_gpt4(privatized_ctx_laplace, answer_span=None)\n","\n","        results_laplace.append(\n","            {\n","                \"question\": q,\n","                \"rating\": rating, # Store rating\n","                \"original_context\": ctx,\n","                \"privatized_context\": privatized_ctx_laplace,\n","                # Removed: \"repaired_context\": repaired_ctx_laplace,\n","            }\n","        )\n","\n","    # Calculate average epsilon per token for this eps_map\n","    average_epsilon_per_token = total_epsilon_sum / total_tokens_count if total_tokens_count > 0 else 0\n","\n","    # Create a pandas DataFrame and save it to CSV with average epsilon in the filename\n","    df_results_laplace = pd.DataFrame(results_laplace)\n","    df_results_laplace.to_csv(f\"AGnews_uniform_laplace_sweep_avg_epsilon_{average_epsilon_per_token:.2f}.csv\", index=False)\n","\n","    print(f\"Processing complete for eps_map {i+1}. Results saved to AGnews_uniform_laplace_sweep_avg_epsilon_{average_epsilon_per_token:.2f}.csv\")\n","\n","print(\"All sweeps complete.\")"],"metadata":{"id":"cPVfta-RN-Er","colab":{"base_uri":"https://localhost:8080/","height":211},"collapsed":true,"outputId":"26da46af-3827-42c5-a6ef-62295d04d672","executionInfo":{"status":"error","timestamp":1763766705653,"user_tz":420,"elapsed":15,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'load_dataset' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1736103995.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0meps_maps_sweep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_eps_maps_with_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_eps_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_eps_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_eps_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ag_news\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test[:50]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_map\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps_maps_sweep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"r5mIUv1-46Oe","executionInfo":{"status":"aborted","timestamp":1763766705656,"user_tz":420,"elapsed":1,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply uniform_polar with special_eps_map = {1: 100, 2: 0, 3: 350, 4: 250}\n","\n","eps_map = {1: 100, 2: 1, 3: 350, 4: 250}\n","\n","ds = load_dataset(\"ag_news\", split=\"test[:50]\")\n","\n","print(f\"Processing with eps_map = {eps_map} for uniform_laplace\")\n","\n","results_laplace = []\n","total_epsilon_sum_laplace = 0\n","total_tokens_count_laplace = 0\n","\n","\n","for ex in ds:\n","    ctx = ex[\"text\"]\n","    q = \"Which category does this news article belong to: World, Sports, Business, or Sci/Tech?\"\n","\n","    rating = ex[\"label\"]\n","\n","    tok_ids, _, groups = partition_tokens_2x2(\n","        context=ctx,\n","        question=q,\n","        tokenizer=tokenizer,\n","        embedding_table=embedding_table,\n","        tau=0.50,\n","    )\n","\n","    # Calculate total epsilon for the sentence based on the special_eps_map\n","    sentence_epsilon_sum = sum(eps_map[group] for group in groups)\n","\n","    # Calculate the average budget per token for this sentence\n","    # avg_budget_for_uniform = sentence_epsilon_sum / len(tok_ids) if len(tok_ids) > 0 else 0\n","\n","    # Create a uniform epsilon map with the calculated average budget\n","    # uniform_eps_map = {1: avg_budget_for_uniform, 2: avg_budget_for_uniform, 3: avg_budget_for_uniform, 4: avg_budget_for_uniform}\n","\n","    # Accumulate total epsilon and token count for uniform_laplace\n","    total_epsilon_sum_laplace += sum(eps_map[group] for group in groups)\n","    total_tokens_count_laplace += len(tok_ids)\n","\n","\n","    privatized_ctx_laplace = apply_stamp_laplace( # Assuming apply_stamp is the uniform laplace function\n","        torch.tensor(tok_ids, device=embedding_table.device),\n","        groups,\n","        embedding_table,\n","        tokenizer,\n","        eps_dir_by_group=eps_map,\n","    )[0]\n","\n","    results_laplace.append(\n","        {\n","            \"question\": q,\n","            \"rating\": rating,\n","            \"original_context\": ctx,\n","            \"privatized_context\": privatized_ctx_laplace,\n","        }\n","    )\n","\n","\n","# Calculate average epsilon per token for uniform_laplace\n","average_epsilon_per_token_laplace = total_epsilon_sum_laplace / total_tokens_count_laplace if total_tokens_count_laplace > 0 else 0\n","\n","df_results_laplace = pd.DataFrame(results_laplace)\n","df_results_laplace.to_csv(f\"AGnews_laplace_special_avg_epsilon_{average_epsilon_per_token_laplace:.2f}.csv\", index=False)\n","\n","print(f\"Processing complete for uniform_laplace case. Results saved to AGnews_laplace_special_avg_epsilon_{average_epsilon_per_token_laplace:.2f}.csv\")"],"metadata":{"id":"CQZEc8d_47BO","executionInfo":{"status":"aborted","timestamp":1763766705664,"user_tz":420,"elapsed":2,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"r_rQEyzN44tM","executionInfo":{"status":"aborted","timestamp":1763766705666,"user_tz":420,"elapsed":1,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Simpler cases"],"metadata":{"id":"GDd27FfC3JuX","executionInfo":{"status":"aborted","timestamp":1763766705667,"user_tz":420,"elapsed":375,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# apply_stamp_allow_inf_laplace: stamp with laplace\n","\n","# Load the Yelp dataset instead of SQuAD\n","ds = load_dataset(\"ag_news\", split=\"test[:50]\")\n","\n","# Update start and end epsilon maps and step\n","start_eps_map = {1: 150, 2: 150, 3: float(\"inf\"), 4: float(\"inf\")}\n","end_eps_map = {1: 550, 2: 550, 3: float(\"inf\"), 4: float(\"inf\")}\n","step_eps_map = {1: 50, 2: 50, 3: 0, 4: 0} # Step size for each key\n","\n","eps_maps_sweep = generate_eps_maps_with_step(start_eps_map, end_eps_map, step_eps_map)\n","\n","\n","for i, eps_map in enumerate(eps_maps_sweep):\n","    print(f\"Processing with eps_map = {eps_map}\")\n","    results_laplace = []\n","    total_epsilon_sum = 0\n","    total_tokens_count = 0\n","\n","    for ex in ds:\n","        # Access review text and rating using correct keys for Yelp dataset\n","        ctx = ex[\"text\"]\n","        q = \"Which category does this news article belong to: World, Sports, Business, or Sci/Tech?\"\n","\n","        rating = ex[\"label\"]  # Access rating\n","\n","        # Use the canonical partitioner\n","        tok_ids, _, groups = partition_tokens_2x2(\n","            context=ctx,\n","            question=q,\n","            tokenizer=tokenizer,\n","            embedding_table=embedding_table,\n","            tau=0.50,  # Using a fixed tau for this sweep\n","        )\n","\n","        # Calculate total epsilon and token count for average calculation\n","        sentence_epsilon_sum = sum(eps_map[group] for group in groups if eps_map[group] != float('inf')) # Only sum finite epsilons\n","        total_epsilon_sum += sentence_epsilon_sum\n","        total_tokens_count += len([g for g in groups if eps_map[g] != float('inf')]) # Only count tokens with finite epsilons\n","\n","\n","        privatized_ctx_laplace = apply_stamp_allow_inf_laplace(\n","            torch.tensor(tok_ids, device=embedding_table.device),\n","            groups,\n","            embedding_table,\n","            tokenizer,\n","            eps_dir_by_group=eps_map,                       # <-- actually used\n","        )[0]\n","\n","        # Removed: Apply repair_coherent_gpt4 as it's designed for QA pairs\n","        # repaired_ctx_laplace = repair_coherent_gpt4(privatized_ctx_laplace, answer_span=None)\n","\n","        results_laplace.append(\n","            {\n","                \"question\": q,\n","                \"rating\": rating, # Store rating\n","                \"original_context\": ctx,\n","                \"privatized_context\": privatized_ctx_laplace,\n","                # Removed: \"repaired_context\": repaired_ctx_laplace,\n","            }\n","        )\n","\n","    # Calculate average epsilon per token for this eps_map (only considering tokens with finite epsilon)\n","    average_epsilon_per_token = total_epsilon_sum / total_tokens_count if total_tokens_count > 0 else 0\n","\n","    # Create a pandas DataFrame and save it to CSV with average epsilon in the filename\n","    # Using a filename appropriate for the Yelp dataset\n","    df_results_laplace = pd.DataFrame(results_laplace)\n","    df_results_laplace.to_csv(f\"AGnews_inf_laplace_sweep_avg_epsilon_{average_epsilon_per_token:.2f}.csv\", index=False)\n","\n","    print(f\"Processing complete for eps_map {i+1}. Results saved to AGnews_inf_laplace_sweep_avg_epsilon_{average_epsilon_per_token:.2f}.csv\")\n","\n","print(\"All sweeps complete.\")"],"metadata":{"id":"t4aQ3CVR0pAk","executionInfo":{"status":"aborted","timestamp":1763766705667,"user_tz":420,"elapsed":373,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}},"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h1Unps_exB86","executionInfo":{"status":"aborted","timestamp":1763766705669,"user_tz":420,"elapsed":375,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QmgX_UtcBE21","executionInfo":{"status":"aborted","timestamp":1763766705676,"user_tz":420,"elapsed":382,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test # 1 only musk out group 1&2 and fill by gpt4\n","\n","eps_map = {1: 0.0, 2: 0.0, 3: float(\"inf\"), 4: float(\"inf\")}\n","\n","# Load the Yelp dataset instead of SQuAD\n","ds = load_dataset(\"ag_news\", split=\"test[:50]\")\n","total = Counter()\n","results_gpt4_fill = [] # List to store results\n","\n","for ex in ds:\n","    # Access review text and rating using correct keys for Yelp dataset\n","    ctx = ex[\"text\"]\n","    q = \"Which category does this news article belong to: World, Sports, Business, or Sci/Tech?\"\n","\n","    rating = ex[\"label\"] # Access rating\n","\n","    tok_ids, _, groups = partition_tokens_2x2(\n","        context=ctx,\n","        question=q,\n","        tokenizer=tokenizer,           # BASE tokenizer\n","        embedding_table=embedding_table,\n","        tau=0.50, # Using a fixed tau for this example\n","    )\n","    counts = Counter(groups); total.update(groups)\n","\n","    # Apply masking and GPT-4 filling\n","    # Note: apply_drop_zero_keep_inf_gpt4fill_ids is designed for QA pairs and might\n","    # require adjustments for optimal performance on reviews.\n","    privatized_ctx, _ = apply_drop_zero_keep_inf_gpt4fill_ids(\n","        token_ids=tok_ids,\n","        groups=groups,\n","        eps_dir_by_group=eps_map,\n","        tokenizer=tokenizer,\n","        openai_client=client,\n","        model=\"gpt-4o-mini\",\n","        temperature=0.2,\n","        top_p=0.9,\n","        context_window=256,\n","        enforce_one_token=True,        # keep exactly 1 base token per drop\n","        # mask_groups={1,3},           # optional override independent of eps_map\n","    )\n","\n","    # Removed: Apply repair_coherent_gpt4 as it's designed for QA pairs\n","    # repaired_ctx = repair_coherent_gpt4(privatized_ctx, answer_span=None)\n","\n","    results_gpt4_fill.append({\n","        \"question\": q,\n","        \"rating\": rating,\n","        \"original_context\": ctx,\n","        \"privatized_context_gpt4_fill\": privatized_ctx,\n","    })\n","\n","\n","# Create a pandas DataFrame and save it to CSV\n","df_results_gpt4_fill = pd.DataFrame(results_gpt4_fill)\n","df_results_gpt4_fill.to_csv(f\"AGnews_gpt4_fill_tau_{tau:.1f}.csv\", index=False)\n","\n","\n","print(\"Original Review:\", ctx)\n","print(\"Rating:\", rating)\n","print(\"Privatized context (Masked and Filled):\", privatized_ctx)\n","# Removed: print(\"Repaired context:\", repaired_ctx)\n","print(f\"Processing complete. Results saved to AGnews_gpt4_fill_tau_{tau:.1f}.csv\")"],"metadata":{"id":"x8TXupNtQG3U","executionInfo":{"status":"aborted","timestamp":1763766705677,"user_tz":420,"elapsed":381,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4ZBjkQzLSldj","executionInfo":{"status":"aborted","timestamp":1763766705678,"user_tz":420,"elapsed":381,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GmtVPio0S6P3","executionInfo":{"status":"aborted","timestamp":1763766705680,"user_tz":420,"elapsed":383,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]}]}