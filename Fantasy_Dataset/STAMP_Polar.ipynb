{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyPve9UrkzdNpKOw4vfdlNLc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# --- Load My Functions ---\n","# Put the file name functions.py under folder\n","import functions\n","from functions import *\n","\n","import GPT_function\n","from GPT_function import *"],"metadata":{"id":"M9hQyGBFlV0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"opy49YOY1Pqy","executionInfo":{"status":"aborted","timestamp":1763766667074,"user_tz":420,"elapsed":240,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"outputs":[],"source":["# Library\n","\n","import torch\n","import math\n","import re\n","import numpy as np\n","import pandas as pd\n","from torch.distributions import Laplace\n","\n","from transformers import AutoTokenizer\n","from transformers import AutoModel\n","from transformers import AutoModelForCausalLM\n","from transformers import GPT2LMHeadModel\n","\n","from datasets import load_dataset\n","from sentence_transformers import SentenceTransformer, util\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from scipy.stats import vonmises_fisher\n","import torch.nn.functional as F\n","\n","from typing import Dict, List, Optional\n","\n","from openai import OpenAI\n","\n","from collections import Counter"]},{"cell_type":"code","source":["# Change this if possible\n","# Also change the one in GPT_function\n","\n","client = OpenAI(api_key=\"Your_API_Key\")  # needs OPENAI_API_KEY\n"],"metadata":{"id":"37Qdf9kEIG6l","executionInfo":{"status":"aborted","timestamp":1763766667075,"user_tz":420,"elapsed":240,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- load only squad v2 validation ---\n","squad = load_dataset(\"Setpember/Fantasy-SQUAD_10\")\n"],"metadata":{"id":"AInEnvGSlVvH","executionInfo":{"status":"aborted","timestamp":1763766667076,"user_tz":420,"elapsed":239,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Load tokenizer and GPT-2 model ---\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\", output_hidden_states=True)\n","embedding_table = model.get_input_embeddings().weight.detach()\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","model.eval()\n","\n","# Base tokenizer pad fix (optional)\n","if getattr(tokenizer, \"pad_token_id\", None) is None and getattr(tokenizer, \"eos_token\", None) is not None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","# Load a light GPT-2 model\n","gpt2_tok = AutoTokenizer.from_pretrained(\"distilgpt2\")\n","if gpt2_tok.pad_token is None:\n","    gpt2_tok.pad_token = gpt2_tok.eos_token\n","gpt2_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(\n","    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",").eval()\n","\n","# --- Extract embedding table ---\n","# Normalize embedding table for search\n","norm_embedding_table = torch.nn.functional.normalize(embedding_table, dim=1)"],"metadata":{"id":"YZecP5w2lVxq","executionInfo":{"status":"aborted","timestamp":1763766667077,"user_tz":420,"elapsed":240,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sOYdjIRM06O9","executionInfo":{"status":"aborted","timestamp":1763766667077,"user_tz":420,"elapsed":239,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Heuristic privacy flags at the word level (_privacy_flag_for_word)\n","# A word is marked private = True if any of these fire:\n","\n","# 1. Looks like email or URL ([A-Z0-9._%+-]+@..., http(s):// or www.).\n","\n","# 2. Contains a digit (covers dates, addresses, ages, years, etc.).\n","\n","# 3. Alphanumeric mix like A12B or user123.\n","\n","# 4. Is a month name/abbr. (e.g., jan, feb, september, …).\n","\n","# 5. Ends with common location suffixes (e.g., -ville, -town, -city, -grad, …).\n","\n","# 6. Proper-noun-ish capitalization or CamelCase chunk.\n","\n","# 7. Starts with @ or # (handles/hashtags).\n","\n","# 8. Long ID-ish tokens (≥6) with underscores/hyphens allowed.\n","\n","# Every subtoken of that word gets the same private value."],"metadata":{"id":"qVMqefXL06UK","executionInfo":{"status":"aborted","timestamp":1763766667078,"user_tz":420,"elapsed":240,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Important? (token-level cosine to the question)\n","\n","# Question centroid\n","\n","# 1. Take the (static) GPT-2 embedding for each question token, L2-normalize each, then average and re-normalize → q.\n","\n","# Token–question similarity\n","\n","# 2. For each context token, take its (static) embedding, L2-normalize, compute cos = dot(token_emb, q).\n","\n","# Threshold by τ, important = (cos ≥ τ), with τ set by --tau (default 0.5).\n","\n","# Note: this is not a contextual encoder; it’s using the GPT-2 embedding table only, so it’s a simple, fast proxy for task relevance."],"metadata":{"id":"Mr1I-2TL1Nql","executionInfo":{"status":"aborted","timestamp":1763766667078,"user_tz":420,"elapsed":240,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This is actual STAMP\n","# This is a play around parameter for importance , default 0.5\n","tau = 0.50\n","\n","@torch.no_grad()\n","def get_task_vector(question: str, tokenizer, embedding_table: torch.Tensor) -> torch.Tensor:\n","    q_ids = tokenizer.encode(question, add_special_tokens=False)\n","    if len(q_ids) == 0:\n","        v = torch.randn(embedding_table.shape[1], device=embedding_table.device)\n","        return F.normalize(v, dim=0)\n","    q_ids_t = torch.tensor(q_ids, dtype=torch.long, device=embedding_table.device)\n","    q_vec = embedding_table[q_ids_t].mean(dim=0)\n","    return F.normalize(q_vec, dim=0)\n","\n","# ---- Light NER-ish fallback (unchanged, just kept here)\n","_CAP  = re.compile(r\"^[A-Z][a-z]{2,}$\")\n","_EMAIL= re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n","_NUM  = re.compile(r\"^\\d{2,}$\")\n","def _clean_piece(p: str) -> str:\n","    return p.lstrip(\"Ġ▁\").lstrip(\"##\")\n","def fallback_ner(tokens):\n","    toks = [_clean_piece(t) for t in tokens]\n","    return [bool(_EMAIL.search(t) or _NUM.match(t) or _CAP.match(t)) for t in toks]\n","\n","# ==== canonical partitioner ====\n","@torch.no_grad()\n","def partition_tokens_2x2(context: str,\n","                         question: str,\n","                         tokenizer,\n","                         embedding_table: torch.Tensor,\n","                         tau: float = tau,\n","                         ner_fn=None):\n","    \"\"\"\n","    Returns: token_ids (List[int]), tokens (List[str]), groups (List[int] in {1,2,3,4})\n","      1 = High-privacy × High-importance\n","      2 = High-privacy × Low-importance\n","      3 = Low-privacy  × High-importance\n","      4 = Low-privacy  × Low-importance\n","    \"\"\"\n","    token_ids = tokenizer.encode(context, add_special_tokens=False)\n","    pieces = tokenizer.convert_ids_to_tokens(token_ids)\n","\n","    # Importance via cosine to task vector\n","    q_vec = get_task_vector(question, tokenizer, embedding_table)     # (d,)\n","    ids_t = torch.tensor(token_ids, dtype=torch.long, device=embedding_table.device)\n","    vecs  = embedding_table[ids_t]                                    # [n,d]\n","    vecs  = F.normalize(vecs, dim=1)\n","    sims  = (vecs @ q_vec)                                            # [n]\n","    high_imp = sims.ge(tau).tolist()\n","\n","    # Privacy via NER-ish heuristic by default\n","    high_priv = (ner_fn or fallback_ner)(pieces)\n","\n","    groups = []\n","    for p, imp in zip(high_priv, high_imp):\n","        if p and imp:         groups.append(1)\n","        elif p and not imp:   groups.append(2)\n","        elif (not p) and imp: groups.append(3)\n","        else:                 groups.append(4)\n","\n","    # Counts per group\n","    cnt = Counter(groups)\n","    total = len(groups)\n","    print(f\"τ={tau}  |  tokens={total}  |  G1={cnt.get(1,0)}  G2={cnt.get(2,0)}  G3={cnt.get(3,0)}  G4={cnt.get(4,0)}\")\n","\n","    return token_ids, pieces, groups\n","\n","\n","# ---- Back-compat alias so existing calls don't break (IMPORTANT):\n","partition_tokens_paper = partition_tokens_2x2\n","\n","# ---- Helper that returns ONLY the groups (for apply_stamp callers)\n","def groups_2x2(context: str,\n","               question: str,\n","               tokenizer,\n","               embedding_table: torch.Tensor,\n","               **kw) -> list[int]:\n","    tok_ids, _, groups = partition_tokens_2x2(context, question, tokenizer, embedding_table, **kw)\n","    return groups, tok_ids"],"metadata":{"id":"DGzP0TsQGj6x","executionInfo":{"status":"aborted","timestamp":1763766667079,"user_tz":420,"elapsed":241,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JxxpGh320pDB","executionInfo":{"status":"aborted","timestamp":1763766667079,"user_tz":420,"elapsed":240,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Unit test on partition"],"metadata":{"id":"P6-8Q7LwK4Sy","executionInfo":{"status":"aborted","timestamp":1763766667080,"user_tz":420,"elapsed":241,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Print tokens by 2×2 group (uses current STAMP.ipynb code) ----\n","def print_tokens_by_group(\n","    context: str,\n","    question: str,\n","    tau: float = None,\n","    *,\n","    labels: dict | None = None,\n","    show_counts: bool = True,\n","    return_buckets: bool = False,\n","):\n","    \"\"\"\n","    Prints the BPE tokens grouped into:\n","      G1: important + private\n","      G2: !important + private\n","      G3: important + !private\n","      G4: !important + !private\n","\n","    Uses the notebook's partitioning function:\n","      - partition_tokens_2x2(context, question, tokenizer, embedding_table, tau=...)\n","        -> (token_ids, pieces, groups)\n","    Falls back to:\n","      - groups_2x2(...) -> (groups, token_ids) + tokenizer.convert_ids_to_tokens(...)\n","    \"\"\"\n","    # default to the notebook's global tau if not provided\n","    if tau is None:\n","        try:\n","            _ = tau  # local arg\n","        except NameError:\n","            pass\n","        tau = globals().get(\"tau\", 0.50)\n","\n","    # Try the main API: partition_tokens_2x2 returns pieces directly\n","    pieces = None\n","    try:\n","        token_ids, pieces, groups = partition_tokens_2x2(\n","            context=context,\n","            question=question,\n","            tokenizer=tokenizer,\n","            embedding_table=embedding_table,\n","            tau=tau,\n","        )\n","    except NameError:\n","        # Back-compat path if only groups_2x2 is defined\n","        groups, token_ids = groups_2x2(\n","            context=context,\n","            question=question,\n","            tokenizer=tokenizer,\n","            embedding_table=embedding_table,\n","            tau=tau,\n","        )\n","    # If pieces not provided, derive from ids\n","    if pieces is None:\n","        pieces = tokenizer.convert_ids_to_tokens(token_ids)\n","\n","    # Bucket tokens\n","    buckets = {1: [], 2: [], 3: [], 4: []}\n","    for tok, g in zip(pieces, groups):\n","        buckets[g].append(tok)\n","\n","    lab = labels or {\n","        1: \"G1 (imp+priv)\",\n","        2: \"G2 (!imp+priv)\",\n","        3: \"G3 (imp+!priv)\",\n","        4: \"G4 (!imp+!priv)\",\n","    }\n","\n","    for g in (1, 2, 3, 4):\n","        header = f\"\\n{lab[g]}\"\n","        if show_counts:\n","            header += f\" — {len(buckets[g])} tokens\"\n","        print(header + \":\")\n","        print(\" \".join(buckets[g]) if buckets[g] else \"—\")\n","\n","    if return_buckets:\n","        return buckets\n"],"metadata":{"id":"ShL8fLJT0pLi","executionInfo":{"status":"aborted","timestamp":1763766667080,"user_tz":420,"elapsed":241,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lets try an example\n","ctx = \"Barack Obama was born in Hawaii on August 4, 1961.\"\n","q   = \"Where was Obama born?\"\n","print_tokens_by_group(ctx, q, tau=0.50)\n","\n","ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\", split=\"validation[:1]\")\n","\n","\n","for ex in ds:\n","    ctx = ex[\"context\"]\n","    q   = ex[\"question\"]\n","\n","    # Use the canonical partitioner\n","    print_tokens_by_group(ctx, q, tau=0.50)\n"],"metadata":{"id":"BLCLGLun0pN7","executionInfo":{"status":"aborted","timestamp":1763766667081,"user_tz":420,"elapsed":242,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}},"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Unit test on Squad"],"metadata":{"id":"aZopyhJM3MCn","executionInfo":{"status":"aborted","timestamp":1763766667081,"user_tz":420,"elapsed":242,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# English as a second language\n","def repair_coherent_gpt4(\n","    text: str,\n","    *,\n","    answer_span: str | None = None,   # keep this exact text unchanged if provided\n","    model: str = \"gpt-4o-mini\",\n","    temperature: float = 0.2,\n","    max_tokens: int = 800,\n","    keep_weird_tokens: bool = True,   # keep nonce tokens (e.g., \"thewoodivum\") as-is\n",") -> str:\n","    \"\"\"\n","    Rewrites 'text' into coherent English, preserving meaning and not adding facts.\n","    If answer_span is provided, it's tagged and MUST remain verbatim.\n","    \"\"\"\n","\n","    # Optionally protect odd tokens so the model doesn’t normalize them away.\n","    protected = set()\n","    if keep_weird_tokens:\n","        # crude heuristic: tokens with digits OR mixed case OR long runs of consonants OR non-ascii\n","        for tok in set(re.findall(r\"[A-Za-z0-9\\u00C0-\\u024F\\u0100-\\u017F\\-']{3,}\", text)):\n","            if (re.search(r\"\\d\", tok)\n","                or (re.search(r\"[A-Z]\", tok) and re.search(r\"[a-z]\", tok))       # CamelCase-ish\n","                or re.search(r\"[^\\x00-\\x7F]\", tok)                               # non-ascii\n","                or re.search(r\"[bcdfghjklmnpqrstvwxyz]{4,}\", tok.lower())):      # consonant run\n","                protected.add(tok)\n","    # Tag protected tokens\n","    tagged = text\n","    for tok in sorted(protected, key=len, reverse=True):\n","        tagged = re.sub(rf\"\\b{re.escape(tok)}\\b\", f\"<ENT>{tok}</ENT>\", tagged)\n","\n","    # Tag answer span (first occurrence) if provided\n","    if answer_span:\n","        idx = tagged.find(answer_span)\n","        if idx != -1:\n","            tagged = tagged[:idx] + \"<ANS>\" + answer_span + \"</ANS>\" + tagged[idx+len(answer_span):]\n","\n","    sys = (\n","        \"You are a careful editor. Rewrite the passage into coherent, grammatical English, \"\n","        \"keeping the original meaning and tone. Do NOT add external facts, \"\n","        # \"do NOT invent names, dates, locations, or entities, and do NOT expand abbreviations. \"\n","        \"Keep unusual or unknown tokens as-is if wrapped in <ENT>...</ENT>. \"\n","        \"If <ANS>...</ANS> appears, keep its contents exactly unchanged and keep it in place. \"\n","        \"Preserve paragraphing; only fix grammar/fluency and minimal function words.\"\n","    )\n","\n","    user = f\"Rewrite the passage below. Return ONLY the rewritten passage (no commentary).\\n\\n{tagged}\"\n","\n","    out = client.chat.completions.create(\n","        model=model,\n","        messages=[{\"role\": \"system\", \"content\": sys},\n","                  {\"role\": \"user\", \"content\": user}],\n","        temperature=temperature,\n","        max_tokens=max_tokens,\n","    )\n","    text_out = (out.choices[0].message.content or \"\").strip()\n","\n","    # Untag\n","    text_out = text_out.replace(\"<ENT>\", \"\").replace(\"</ENT>\", \"\")\n","    if answer_span:\n","        text_out = text_out.replace(\"<ANS>\", \"\").replace(\"</ANS>\", \"\")\n","\n","    return text_out\n","\n","# Unit test on garabage\n","noisy = (\"Beneath a vaulted ceiling of driftwood ribs, theroomivum Marest convened to read thescriptthe.\")\n","clean = repair_coherent_gpt4(noisy, answer_span=None)\n","print(clean)"],"metadata":{"id":"EcT2TPUCSgkV","executionInfo":{"status":"error","timestamp":1763766667184,"user_tz":420,"elapsed":96,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}},"colab":{"base_uri":"https://localhost:8080/","height":315},"outputId":"611f71f9-8fa6-48b5-e33b-dc79e38502e7"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 're' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1911572999.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# Unit test on garabage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mnoisy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Beneath a vaulted ceiling of driftwood ribs, theroomivum Marest convened to read thescriptthe.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mclean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepair_coherent_gpt4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_span\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1911572999.py\u001b[0m in \u001b[0;36mrepair_coherent_gpt4\u001b[0;34m(text, answer_span, model, temperature, max_tokens, keep_weird_tokens)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeep_weird_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# crude heuristic: tokens with digits OR mixed case OR long runs of consonants OR non-ascii\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"[A-Za-z0-9\\u00C0-\\u024F\\u0100-\\u017F\\-']{3,}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             if (re.search(r\"\\d\", tok)\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"[A-Z]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"[a-z]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# CamelCase-ish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 're' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"rlyStZ2DSvD4","executionInfo":{"status":"aborted","timestamp":1763766667186,"user_tz":420,"elapsed":344,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply repair_coherent_gpt4 after apply_stamp with polar\n","results = []\n","\n","ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\", split=\"validation[:3]\")\n","eps_map = {1:200, 2:100, 3:500, 4:400}\n","\n","for ex in ds:\n","    ctx = ex[\"context\"]\n","    q   = ex[\"question\"]\n","    answers = ex[\"answers\"][\"text\"] # Extract answers\n","\n","    # Use the canonical partitioner\n","    tok_ids, _, groups = partition_tokens_2x2(\n","        context=ctx,\n","        question=q,\n","        tokenizer=tokenizer,\n","        embedding_table=embedding_table,\n","        tau=0.50,                 # or tune\n","        # ner_fn=your_spacy_ner   # optional: plug a heavier NER here\n","    )\n","\n","    privatized_ctx = apply_stamp(\n","        torch.tensor(tok_ids, device=embedding_table.device),\n","        groups,\n","        embedding_table,\n","        tokenizer,\n","        eps_dir_by_group=eps_map,                       # <-- actually used\n","    )[0]\n","\n","    # Apply repair_coherent_gpt4\n","    repaired_ctx = repair_coherent_gpt4(privatized_ctx, answer_span=None)\n","\n","    results.append({\n","        \"question\": q,\n","        \"answers\": answers,\n","        \"original_context\": ctx,\n","        \"privatized_context\": privatized_ctx,\n","        \"repaired_context\": repaired_ctx\n","    })\n","\n","# Create a pandas DataFrame and save it to CSV\n","df_results = pd.DataFrame(results)\n","df_results.to_csv(\"squad_stamp_polar.csv\", index=False)\n","\n","print(\"Processing complete. Results saved to squad_stamp_polar.csv\")"],"metadata":{"collapsed":true,"id":"InEMNe7Z3T8Z","executionInfo":{"status":"aborted","timestamp":1763766667186,"user_tz":420,"elapsed":344,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply repair_coherent_gpt4 after apply_stamp with polar for different tau values\n","\n","tau_values = [0.3, 0.4, 0.5, 0.6, 0.7]\n","eps_map = {1: 200, 2: 100, 3: 500, 4: 400}\n","\n","for tau in tau_values:\n","    print(f\"Processing with tau = {tau}\")\n","    results = []\n","\n","    ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\", split=\"validation\")\n","\n","    for ex in ds:\n","        ctx = ex[\"context\"]\n","        q = ex[\"question\"]\n","        answers = ex[\"answers\"][\"text\"]  # Extract answers\n","\n","        # Use the canonical partitioner\n","        tok_ids, _, groups = partition_tokens_2x2(\n","            context=ctx,\n","            question=q,\n","            tokenizer=tokenizer,\n","            embedding_table=embedding_table,\n","            tau=tau,  # Use the current tau value\n","        )\n","\n","        privatized_ctx = apply_stamp(\n","            torch.tensor(tok_ids, device=embedding_table.device),\n","            groups,\n","            embedding_table,\n","            tokenizer,\n","            eps_dir_by_group=eps_map,  # <-- actually used\n","        )[0]\n","\n","        # Apply repair_coherent_gpt4\n","        repaired_ctx = repair_coherent_gpt4(privatized_ctx, answer_span=None)\n","\n","        results.append(\n","            {\n","                \"question\": q,\n","                \"answers\": answers,\n","                \"original_context\": ctx,\n","                \"privatized_context\": privatized_ctx,\n","                \"repaired_context\": repaired_ctx,\n","            }\n","        )\n","\n","    # Create a pandas DataFrame and save it to CSV with tau value in the filename\n","    df_results = pd.DataFrame(results)\n","    df_results.to_csv(f\"squad_stamp_polar_tau_{tau:.1f}.csv\", index=False)\n","\n","    print(f\"Processing complete for tau = {tau}. Results saved to squad_stamp_polar_tau_{tau:.1f}.csv\")\n","\n","print(\"All processing complete.\")"],"metadata":{"id":"nn1Q--953T_J","collapsed":true,"executionInfo":{"status":"aborted","timestamp":1763766667187,"user_tz":420,"elapsed":343,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Qy9NQ0GUGDWo","executionInfo":{"status":"aborted","timestamp":1763766667187,"user_tz":420,"elapsed":343,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply repair_coherent_gpt4 after apply_stamp for different privacy maps\n","\n","def generate_eps_maps_with_step(start_map, end_map, step):\n","    \"\"\"Generates a list of eps_map dictionaries with values incremented by step.\"\"\"\n","    maps = []\n","    current_map = start_map.copy()\n","    while all(current_map.get(k, float('inf')) <= end_map.get(k, float('inf')) for k in end_map):\n","        maps.append(current_map.copy())\n","        next_map = {}\n","        for key in end_map:\n","            start_val = start_map.get(key, float('inf'))\n","            end_val = end_map.get(key, float('inf'))\n","            current_val = current_map.get(key, float('inf'))\n","            if current_val == float('inf'):\n","                 next_map[key] = float('inf')\n","            else:\n","                next_map[key] = current_val + step[key] if key in step else current_val + step.get('default', 0)\n","                if next_map[key] > end_val and end_val != float('inf'):\n","                    next_map[key] = end_val\n","        current_map = next_map\n","        # Break if no values were incremented (handles cases where start == end or step is 0)\n","        if current_map == maps[-1]:\n","            break\n","\n","    # Ensure the end_map is included if it wasn't reached exactly\n","    if maps and maps[-1] != end_map:\n","        maps.append(end_map.copy())\n","\n","    return maps\n","\n","start_eps_map = {1: 150, 2: 50, 3: 450, 4: 350}\n","end_eps_map = {1: 350, 2: 250, 3: 650, 4: 550}\n","step_eps_map = {1: 50, 2: 50, 3: 50, 4: 50} # Step size for each key\n","\n","eps_maps_sweep = generate_eps_maps_with_step(start_eps_map, end_eps_map, step_eps_map)\n","\n","ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\", split=\"validation\")\n","\n","for i, eps_map in enumerate(eps_maps_sweep):\n","    print(f\"Processing with eps_map = {eps_map}\")\n","    results_polar = [] # Changed variable name\n","    total_epsilon_sum = 0\n","    total_tokens_count = 0\n","\n","    for ex in ds:\n","        ctx = ex[\"context\"]\n","        q = ex[\"question\"]\n","        answers = ex[\"answers\"][\"text\"]  # Extract answers\n","\n","        # Use the canonical partitioner\n","        tok_ids, _, groups = partition_tokens_2x2(\n","            context=ctx,\n","            question=q,\n","            tokenizer=tokenizer,\n","            embedding_table=embedding_table,\n","            tau=0.50,  # Using a fixed tau for this sweep\n","        )\n","\n","        # Calculate total epsilon and token count for average calculation\n","        sentence_epsilon_sum = sum(eps_map[group] for group in groups)\n","        total_epsilon_sum += sentence_epsilon_sum\n","        total_tokens_count += len(tok_ids)\n","\n","        # Changed from apply_stamp_laplace to apply_stamp\n","        privatized_ctx_polar = apply_stamp( # Changed variable name\n","            torch.tensor(tok_ids, device=embedding_table.device),\n","            groups,\n","            embedding_table,\n","            tokenizer,\n","            eps_dir_by_group=eps_map,  # <-- actually used\n","        )[0]\n","\n","        # Apply repair_coherent_gpt4\n","        repaired_ctx_polar = repair_coherent_gpt4(privatized_ctx_polar, answer_span=None) # Changed variable name\n","\n","        results_polar.append( # Changed variable name\n","            {\n","                \"question\": q,\n","                \"answers\": answers,\n","                \"original_context\": ctx,\n","                \"privatized_context\": privatized_ctx_polar, # Changed variable name\n","                \"repaired_context\": repaired_ctx_polar, # Changed variable name\n","            }\n","        )\n","\n","    # Calculate average epsilon per token for this eps_map\n","    average_epsilon_per_token = total_epsilon_sum / total_tokens_count if total_tokens_count > 0 else 0\n","\n","    # Create a pandas DataFrame and save it to CSV with average epsilon in the filename\n","    df_results_polar = pd.DataFrame(results_polar) # Changed variable name\n","    df_results_polar.to_csv(f\"squad_stamp_polar_sweep_avg_epsilon_{average_epsilon_per_token:.2f}.csv\", index=False) # Changed filename\n","\n","    print(f\"Processing complete for eps_map {i+1}. Results saved to squad_stamp_polar_sweep_avg_epsilon_{average_epsilon_per_token:.2f}.csv\") # Changed filename\n","\n","print(\"All sweeps complete.\")"],"metadata":{"id":"FNuQtz10EWnb","collapsed":true,"executionInfo":{"status":"aborted","timestamp":1763766667188,"user_tz":420,"elapsed":343,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"N6hrgwT5bS51","executionInfo":{"status":"aborted","timestamp":1763766667188,"user_tz":420,"elapsed":343,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"da77c72c","collapsed":true,"executionInfo":{"status":"aborted","timestamp":1763766667196,"user_tz":420,"elapsed":351,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"source":["# Apply repair_coherent_gpt4 after apply_stamp_laplace\n","results_laplace = []\n","\n","ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\", split=\"validation[:3]\")\n","eps_map = {1:200, 2:100, 3:500, 4:400}\n","\n","for ex in ds:\n","    ctx = ex[\"context\"]\n","    q   = ex[\"question\"]\n","    answers = ex[\"answers\"][\"text\"] # Extract answers\n","\n","    # Use the canonical partitioner\n","    tok_ids, _, groups = partition_tokens_2x2(\n","        context=ctx,\n","        question=q,\n","        tokenizer=tokenizer,\n","        embedding_table=embedding_table,\n","        tau=0.50,                 # or tune\n","        # ner_fn=your_spacy_ner   # optional: plug a heavier NER here\n","    )\n","\n","    privatized_ctx_laplace = apply_stamp_laplace(\n","        torch.tensor(tok_ids, device=embedding_table.device),\n","        groups,\n","        embedding_table,\n","        tokenizer,\n","        eps_dir_by_group=eps_map,                       # <-- actually used\n","    )[0]\n","\n","    # Apply repair_coherent_gpt4\n","    repaired_ctx_laplace = repair_coherent_gpt4(privatized_ctx_laplace, answer_span=None)\n","\n","    results_laplace.append({\n","        \"question\": q,\n","        \"answers\": answers,\n","        \"original_context\": ctx,\n","        \"privatized_context\": privatized_ctx_laplace,\n","        \"repaired_context\": repaired_ctx_laplace\n","    })\n","\n","# Create a pandas DataFrame and save it to CSV\n","df_results_laplace = pd.DataFrame(results_laplace)\n","df_results_laplace.to_csv(f\"squad_stamp_Laplace_tau_{tau:.1f}.csv\", index=False)\n","\n","print(\"Processing complete. Results saved to squad_stamp_laplace_tau.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"A5BlPj9zEWqP","executionInfo":{"status":"aborted","timestamp":1763766667198,"user_tz":420,"elapsed":353,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cPVfta-RN-Er","executionInfo":{"status":"aborted","timestamp":1763766667199,"user_tz":420,"elapsed":354,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply repair_coherent_gpt4 after uniform with polar\n","results = []\n","\n","ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\", split=\"validation\")\n","eps_map = {1:200, 2:100, 3:500, 4:400}\n","\n","for ex in ds:\n","    ctx = ex[\"context\"]\n","    q   = ex[\"question\"]\n","    answers = ex[\"answers\"][\"text\"] # Extract answers\n","\n","    # Use the canonical partitioner\n","    tok_ids, _, groups = partition_tokens_2x2(\n","        context=ctx,\n","        question=q,\n","        tokenizer=tokenizer,\n","        embedding_table=embedding_table,\n","        tau=0.50,                 # or tune\n","        # ner_fn=your_spacy_ner   # optional: plug a heavier NER here\n","    )\n","\n","    # Calculate the total privacy budget per sentence\n","    total_sentence_budget = sum(eps_map[group] for group in groups)\n","\n","    # Calculate the total number of tokens in the sentence\n","    total_tokens = len(tok_ids)\n","\n","    # Calculate the average token privacy budget\n","    average_token_budget = total_sentence_budget / total_tokens if total_tokens > 0 else 0\n","\n","    print(f\"Average token privacy budget for sentence: {average_token_budget:.2f}\")\n","\n","    new_eps_map = {1:average_token_budget, 2:average_token_budget, 3:average_token_budget, 4:average_token_budget}\n","\n","    privatized_ctx = apply_stamp(\n","        torch.tensor(tok_ids, device=embedding_table.device),\n","        groups,\n","        embedding_table,\n","        tokenizer,\n","        eps_dir_by_group=eps_map,                       # <-- actually used\n","    )[0]\n","\n","    # Apply repair_coherent_gpt4\n","    repaired_ctx = repair_coherent_gpt4(privatized_ctx, answer_span=None)\n","\n","    results.append({\n","        \"question\": q,\n","        \"answers\": answers,\n","        \"original_context\": ctx,\n","        \"privatized_context\": privatized_ctx,\n","        \"repaired_context\": repaired_ctx\n","    })\n","\n","# Create a pandas DataFrame and save it to CSV\n","df_results = pd.DataFrame(results)\n","df_results.to_csv(\"squad_Uniform_polar.csv\", index=False)\n","\n","print(\"Processing complete. Results saved to squad_Uniform_polar.csv\")"],"metadata":{"id":"DQLvs27DOCjL","executionInfo":{"status":"aborted","timestamp":1763766667199,"user_tz":420,"elapsed":354,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply repair_coherent_gpt4 after apply_stamp for different privacy maps\n","\n","def generate_eps_maps_with_step(start_map, end_map, step):\n","    \"\"\"Generates a list of eps_map dictionaries with values incremented by step.\"\"\"\n","    maps = []\n","    current_map = start_map.copy()\n","    while all(current_map.get(k, float('inf')) <= end_map.get(k, float('inf')) for k in end_map):\n","        maps.append(current_map.copy())\n","        next_map = {}\n","        for key in end_map:\n","            start_val = start_map.get(key, float('inf'))\n","            end_val = end_map.get(key, float('inf'))\n","            current_val = current_map.get(key, float('inf'))\n","            if current_val == float('inf'):\n","                 next_map[key] = float('inf')\n","            else:\n","                next_map[key] = current_val + step[key] if key in step else current_val + step.get('default', 0)\n","                if next_map[key] > end_val and end_val != float('inf'):\n","                    next_map[key] = end_val\n","        current_map = next_map\n","        # Break if no values were incremented (handles cases where start == end or step is 0)\n","        if current_map == maps[-1]:\n","            break\n","\n","    # Ensure the end_map is included if it wasn't reached exactly\n","    if maps and maps[-1] != end_map:\n","        maps.append(end_map.copy())\n","\n","    return maps\n","\n","start_eps_map = {1: 150, 2: 50, 3: 450, 4: 350}\n","end_eps_map = {1: 350, 2: 250, 3: 650, 4: 550}\n","step_eps_map = {1: 50, 2: 50, 3: 50, 4: 50} # Step size for each key\n","\n","eps_maps_sweep = generate_eps_maps_with_step(start_eps_map, end_eps_map, step_eps_map)\n","\n","ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\", split=\"validation\")\n","\n","for i, eps_map in enumerate(eps_maps_sweep):\n","    print(f\"Processing with eps_map = {eps_map}\")\n","    results_polar = []\n","    total_epsilon_sum = 0\n","    total_tokens_count = 0\n","\n","    for ex in ds:\n","        ctx = ex[\"context\"]\n","        q = ex[\"question\"]\n","        answers = ex[\"answers\"][\"text\"]  # Extract answers\n","\n","        # Use the canonical partitioner\n","        tok_ids, _, groups = partition_tokens_2x2(\n","            context=ctx,\n","            question=q,\n","            tokenizer=tokenizer,\n","            embedding_table=embedding_table,\n","            tau=0.50,  # Using a fixed tau for this sweep\n","        )\n","\n","        # Calculate total epsilon and token count for average calculation\n","        sentence_epsilon_sum = sum(eps_map[group] for group in groups)\n","        total_epsilon_sum += sentence_epsilon_sum\n","        total_tokens_count += len(tok_ids)\n","\n","        # Calculate and print the pre-token average budget for each sentence\n","        pre_token_average_budget = sentence_epsilon_sum / len(tok_ids) if len(tok_ids) > 0 else 0\n","        print(f\"Pre-token average budget for this sentence: {pre_token_average_budget:.2f}\")\n","\n","        # Create a new eps_map for uniform budget for this sentence\n","        uniform_eps_map = {1: pre_token_average_budget, 2: pre_token_average_budget, 3: pre_token_average_budget, 4: pre_token_average_budget}\n","\n","\n","        privatized_ctx_polar = apply_stamp(\n","            torch.tensor(tok_ids, device=embedding_table.device),\n","            groups,\n","            embedding_table,\n","            tokenizer,\n","            eps_dir_by_group=uniform_eps_map,  # <-- actually used\n","        )[0]\n","\n","        # Apply repair_coherent_gpt4\n","        repaired_ctx_polar = repair_coherent_gpt4(privatized_ctx_polar, answer_span=None)\n","\n","        results_polar.append(\n","            {\n","                \"question\": q,\n","                \"answers\": answers,\n","                \"original_context\": ctx,\n","                \"privatized_context\": privatized_ctx_polar,\n","                \"repaired_context\": repaired_ctx_polar,\n","            }\n","        )\n","\n","    # Calculate average epsilon per token for this eps_map\n","    average_epsilon_per_token = total_epsilon_sum / total_tokens_count if total_tokens_count > 0 else 0\n","\n","    # Create a pandas DataFrame and save it to CSV with average epsilon in the filename\n","    df_results_polar = pd.DataFrame(results_polar)\n","    df_results_polar.to_csv(f\"squad_uniform_polar_sweep_avg_epsilon_{average_epsilon_per_token:.2f}.csv\", index=False)\n","\n","    print(f\"Processing complete for eps_map {i+1}. Results saved to squad_uniform_polar_sweep_avg_epsilon_{average_epsilon_per_token:.2f}.csv\")\n","\n","print(\"All sweeps complete.\")"],"metadata":{"id":"airkTh1jGigt","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1763766667233,"user_tz":420,"elapsed":20,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}},"outputId":"5972463a-c93c-4833-ec29-b12b2bd990c6","collapsed":true},"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'load_dataset' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1117085061.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0meps_maps_sweep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_eps_maps_with_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_eps_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_eps_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_eps_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Setpember/Fantasy-SQUAD_10\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_map\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps_maps_sweep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"HKr-1LlvGijM","executionInfo":{"status":"aborted","timestamp":1763766667219,"user_tz":420,"elapsed":371,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nlifl9aMTVf-","executionInfo":{"status":"aborted","timestamp":1763766667220,"user_tz":420,"elapsed":372,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply repair_coherent_gpt4 after uniform with laplace\n","results = []\n","\n","ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\", split=\"validation\")\n","eps_map = {1:200, 2:100, 3:500, 4:400}\n","\n","for ex in ds:\n","    ctx = ex[\"context\"]\n","    q   = ex[\"question\"]\n","    answers = ex[\"answers\"][\"text\"] # Extract answers\n","\n","    # Use the canonical partitioner\n","    tok_ids, _, groups = partition_tokens_2x2(\n","        context=ctx,\n","        question=q,\n","        tokenizer=tokenizer,\n","        embedding_table=embedding_table,\n","        tau=0.50,                 # or tune\n","        # ner_fn=your_spacy_ner   # optional: plug a heavier NER here\n","    )\n","\n","    # Calculate the total privacy budget per sentence\n","    total_sentence_budget = sum(eps_map[group] for group in groups)\n","\n","    # Calculate the total number of tokens in the sentence\n","    total_tokens = len(tok_ids)\n","\n","    # Calculate the average token privacy budget\n","    average_token_budget = total_sentence_budget / total_tokens if total_tokens > 0 else 0\n","\n","    print(f\"Average token privacy budget for sentence: {average_token_budget:.2f}\")\n","\n","    new_eps_map = {1:average_token_budget, 2:average_token_budget, 3:average_token_budget, 4:average_token_budget}\n","\n","    privatized_ctx = apply_stamp_laplace(\n","        torch.tensor(tok_ids, device=embedding_table.device),\n","        groups,\n","        embedding_table,\n","        tokenizer,\n","        eps_dir_by_group=eps_map,                       # <-- actually used\n","    )[0]\n","\n","    # Apply repair_coherent_gpt4\n","    repaired_ctx = repair_coherent_gpt4(privatized_ctx, answer_span=None)\n","\n","    results.append({\n","        \"question\": q,\n","        \"answers\": answers,\n","        \"original_context\": ctx,\n","        \"privatized_context\": privatized_ctx,\n","        \"repaired_context\": repaired_ctx\n","    })\n","\n","# Create a pandas DataFrame and save it to CSV\n","df_results = pd.DataFrame(results)\n","df_results.to_csv(\"squad_Uniform_Laplace.csv\", index=False)\n","\n","print(\"Processing complete. Results saved to squad_Uniform_Laplace.csv\")"],"metadata":{"id":"tvCrWmN-hl6e","executionInfo":{"status":"aborted","timestamp":1763766667221,"user_tz":420,"elapsed":371,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Simpler cases"],"metadata":{"id":"GDd27FfC3JuX","executionInfo":{"status":"aborted","timestamp":1763766667234,"user_tz":420,"elapsed":384,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# apply_stamp_allow_inf_polar: stamp with polar\n","\n","ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\", split=\"validation[:3]\")\n","eps_map = {1:200.0, 2:100.0, 3:float(\"inf\"), 4:float(\"inf\")}\n","\n","for ex in ds:\n","    ctx = ex[\"context\"]\n","    q   = ex[\"question\"]\n","\n","    # Use the canonical partitioner\n","    tok_ids, _, groups = partition_tokens_2x2(\n","        context=ctx,\n","        question=q,\n","        tokenizer=tokenizer,\n","        embedding_table=embedding_table,\n","        tau=0.50,                 # or tune\n","        # ner_fn=your_spacy_ner   # optional: plug a heavier NER here\n","    )\n","\n","    privatized_ctx = apply_stamp_allow_inf_polar(\n","        torch.tensor(tok_ids, device=embedding_table.device),\n","        groups,\n","        embedding_table,\n","        tokenizer,\n","        eps_dir_by_group=eps_map,                       # <-- actually used\n","    )[0]\n","\n","    # Apply repair_coherent_gpt4\n","    repaired_ctx = repair_coherent_gpt4(privatized_ctx, answer_span=None)\n","\n","\n","    print(\"Q:\", q)\n","    print(\"Privatized context (Polar):\", privatized_ctx)\n","    print(\"Repaired context:\", repaired_ctx)"],"metadata":{"id":"81i26NlqceGW","executionInfo":{"status":"aborted","timestamp":1763766667234,"user_tz":420,"elapsed":383,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7xTezHBmwAw_","executionInfo":{"status":"aborted","timestamp":1763766667235,"user_tz":420,"elapsed":384,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"7ea1556a","executionInfo":{"status":"error","timestamp":1763766667255,"user_tz":420,"elapsed":5,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}},"outputId":"8b4e1558-b05b-4702-fad6-d8d6e4f3b299"},"source":["# apply_stamp_allow_inf_polar: stamp with polar\n","\n","ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\", split=\"validation\") # Using full validation set\n","\n","def generate_eps_maps_with_step(start_map, end_map, step):\n","    \"\"\"Generates a list of eps_map dictionaries with values incremented by step.\"\"\"\n","    maps = []\n","    current_map = start_map.copy()\n","    while all(current_map.get(k, float('inf')) <= end_map.get(k, float('inf')) for k in end_map):\n","        maps.append(current_map.copy())\n","        next_map = {}\n","        for key in end_map:\n","            start_val = start_map.get(key, float('inf'))\n","            end_val = end_map.get(key, float('inf'))\n","            current_val = current_map.get(key, float('inf'))\n","            if current_val == float('inf'):\n","                 next_map[key] = float('inf')\n","            else:\n","                next_map[key] = current_val + step[key] if key in step else current_val + step.get('default', 0)\n","                if next_map[key] > end_val and end_val != float('inf'):\n","                    next_map[key] = end_val\n","        current_map = next_map\n","        # Break if no values were incremented (handles cases where start == end or step is 0)\n","        if current_map == maps[-1]:\n","            break\n","\n","    # Ensure the end_map is included if it wasn't reached exactly\n","    if maps and maps[-1] != end_map:\n","        maps.append(end_map.copy())\n","\n","    return maps\n","\n","# Update start and end epsilon maps and step\n","start_eps_map = {1: 150, 2: 150, 3: float(\"inf\"), 4: float(\"inf\")}\n","end_eps_map = {1: 550, 2: 550, 3: float(\"inf\"), 4: float(\"inf\")}\n","step_eps_map = {1: 50, 2: 50, 3: 0, 4: 0} # Step size for each key\n","\n","eps_maps_sweep = generate_eps_maps_with_step(start_eps_map, end_eps_map, step_eps_map)\n","\n","\n","for i, eps_map in enumerate(eps_maps_sweep):\n","    print(f\"Processing with eps_map = {eps_map}\")\n","    results_polar = []\n","    total_epsilon_sum = 0\n","    total_tokens_count = 0\n","\n","    for ex in ds:\n","        ctx = ex[\"context\"]\n","        q = ex[\"question\"]\n","        answers = ex[\"answers\"][\"text\"]  # Extract answers\n","\n","\n","        # Use the canonical partitioner\n","        tok_ids, _, groups = partition_tokens_2x2(\n","            context=ctx,\n","            question=q,\n","            tokenizer=tokenizer,\n","            embedding_table=embedding_table,\n","            tau=0.50,  # Using a fixed tau for this sweep\n","        )\n","\n","        # Calculate total epsilon and token count for average calculation, skipping inf values\n","        sentence_epsilon_sum = sum(eps_map[group] for group in groups if eps_map[group] != float('inf'))\n","        total_epsilon_sum += sentence_epsilon_sum\n","        total_tokens_count += len([tok for tok, group in zip(tok_ids, groups) if eps_map[group] != float('inf')])\n","\n","\n","        privatized_ctx_polar = apply_stamp_allow_inf_polar(\n","            torch.tensor(tok_ids, device=embedding_table.device),\n","            groups,\n","            embedding_table,\n","            tokenizer,\n","            eps_dir_by_group=eps_map,                       # <-- actually used\n","        )[0]\n","\n","        # Apply repair_coherent_gpt4\n","        repaired_ctx_polar = repair_coherent_gpt4(privatized_ctx_polar, answer_span=None)\n","\n","        results_polar.append(\n","            {\n","                \"question\": q,\n","                \"answers\": answers,\n","                \"original_context\": ctx,\n","                \"privatized_context\": privatized_ctx_polar,\n","                \"repaired_context\": repaired_ctx_polar,\n","            }\n","        )\n","\n","    # Calculate average epsilon per token for this eps_map, excluding tokens with inf epsilon\n","    average_epsilon_per_token = total_epsilon_sum / total_tokens_count if total_tokens_count > 0 else 0\n","\n","\n","    # Create a pandas DataFrame and save it to CSV with average epsilon in the filename\n","    df_results_polar = pd.DataFrame(results_polar)\n","    df_results_polar.to_csv(f\"squad_stamp_polar_sweep_inf_avg_epsilon_{average_epsilon_per_token:.2f}.csv\", index=False)\n","\n","\n","    print(f\"Processing complete for eps_map {i+1}. Results saved to squad_stamp_polar_sweep_inf_avg_epsilon_{average_epsilon_per_token:.2f}.csv\")\n","\n","print(\"All sweeps complete.\")"],"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'load_dataset' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2698158710.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# apply_stamp_allow_inf_polar: stamp with polar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Setpember/Fantasy-SQUAD_10\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Using full validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_eps_maps_with_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OmawoubFwAza","executionInfo":{"status":"aborted","timestamp":1763766667254,"user_tz":420,"elapsed":401,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aIIsqw-iwA10","executionInfo":{"status":"aborted","timestamp":1763766667256,"user_tz":420,"elapsed":403,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_vmYnnddwA4c","executionInfo":{"status":"aborted","timestamp":1763766667256,"user_tz":420,"elapsed":403,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# apply_stamp_allow_inf_laplace: stamp with laplace\n","\n","ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\", split=\"validation[:3]\")\n","eps_map = {1:200.0, 2:100.0, 3:float(\"inf\"), 4:float(\"inf\")}\n","\n","for ex in ds:\n","    ctx = ex[\"context\"]\n","    q   = ex[\"question\"]\n","\n","    # Use the canonical partitioner\n","    tok_ids, _, groups = partition_tokens_2x2(\n","        context=ctx,\n","        question=q,\n","        tokenizer=tokenizer,\n","        embedding_table=embedding_table,\n","        tau=0.50,                 # or tune\n","        # ner_fn=your_spacy_ner   # optional: plug a heavier NER here\n","    )\n","\n","    privatized_ctx = apply_stamp_allow_inf_laplace(\n","        torch.tensor(tok_ids, device=embedding_table.device),\n","        groups,\n","        embedding_table,\n","        tokenizer,\n","        eps_dir_by_group=eps_map,                       # <-- actually used\n","    )[0]\n","\n","    # Apply repair_coherent_gpt4\n","    repaired_ctx = repair_coherent_gpt4(privatized_ctx, answer_span=None)\n","\n","    print(\"Q:\", q)\n","    print(\"Privatized context (Polar):\", privatized_ctx)\n","    print(\"Repaired context:\", repaired_ctx)"],"metadata":{"id":"H25WnD-_mMVa","executionInfo":{"status":"aborted","timestamp":1763766667257,"user_tz":420,"elapsed":402,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"t4aQ3CVR0pAk","executionInfo":{"status":"aborted","timestamp":1763766667266,"user_tz":420,"elapsed":411,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kZ44Y8mv0pFg","executionInfo":{"status":"aborted","timestamp":1763766667267,"user_tz":420,"elapsed":412,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # test # 0: drop ε=0, keep ε=+∞, pass-through otherwise\n","\n","ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\", split=\"validation[:3]\")\n","\n","eps_map = {1: 0.0, 2:0.0, 3: float(\"inf\"), 4: float(\"inf\")}\n","\n","total = Counter()\n","\n","for ex in ds:\n","    ctx, q = ex[\"context\"], ex[\"question\"]\n","\n","    # 1) Partition once (for reporting)\n","    tok_ids, _, groups = partition_tokens_2x2(\n","        context=ctx,\n","        question=q,\n","        tokenizer=tokenizer,\n","        embedding_table=embedding_table,\n","        tau=0.30,\n","    )\n","    counts = Counter(groups); total.update(groups)\n","\n","    # 2) Apply zero-drop policy\n","    privatized_ctx = apply_drop_zero_keep_inf(\n","        token_ids=torch.tensor(tok_ids, device=embedding_table.device),\n","        group_assignments=groups,\n","        tokenizer=tokenizer,\n","        eps_dir_by_group=eps_map,\n","    )[0]\n","\n","\n","    # Apply repair_coherent_gpt4\n","    repaired_ctx = repair_coherent_gpt4(privatized_ctx, answer_span=None)\n","\n","    print(\"Q:\", q)\n","    print(\"Privatized context (Polar):\", privatized_ctx)\n","    print(\"Repaired context:\", repaired_ctx)"],"metadata":{"id":"pealANrD8h9i","executionInfo":{"status":"aborted","timestamp":1763766667268,"user_tz":420,"elapsed":411,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hflHaUmY88dH","executionInfo":{"status":"aborted","timestamp":1763766667269,"user_tz":420,"elapsed":412,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test # 1 only musk out group 1&2 and fill by gpt2\n","\n","ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\", split=\"validation[:3]\")\n","\n","# Example policy: G1 → ε=0 (drop+fill), G4 → ε=∞ (keep), G2/G3 → finite (pass-through)\n","eps_map = {1: 0, 2: 0, 3: float(\"inf\"), 4: float(\"inf\")}\n","\n","total = Counter()\n","\n","for ex in ds:\n","    ctx, q = ex[\"context\"], ex[\"question\"]\n","\n","    # 1) Partition (for reporting and to align groups with BASE tokenizer)\n","    tok_ids, _, groups = partition_tokens_2x2(\n","        context=ctx,\n","        question=q,\n","        tokenizer=tokenizer,              # BASE tokenizer (same one used by your embeddings)\n","        embedding_table=embedding_table,\n","        tau=0.50,\n","    )\n","    counts = Counter(groups); total.update(groups)\n","\n","    # 2) Apply drop+GPT-2 fill (IDs variant; returns text + ids)\n","    # some copied form SD code...\n","\n","    privatized_ctx, _ = apply_drop_zero_keep_inf_gpt2fill_ids(\n","        token_ids=tok_ids,\n","        groups=groups,\n","        eps_dir_by_group=eps_map,\n","        tokenizer=tokenizer,              # BASE tokenizer\n","        gpt2_tok=gpt2_tok,                # GPT-2 tokenizer\n","        gpt2_model=gpt2_model,            # GPT-2 model\n","        deterministic=True,               # set False for sampling\n","        temperature=0.2,\n","        top_p=0.9, # or top k=50\n","        context_window=256,\n","    )\n","\n","    # Apply repair_coherent_gpt4\n","    repaired_ctx = repair_coherent_gpt4(privatized_ctx, answer_span=None)\n","\n","\n","    print(\"Q:\", q)\n","    print(\"Privatized context (Polar):\", privatized_ctx)\n","    print(\"Repaired context:\", repaired_ctx)\n"],"metadata":{"id":"gGDBttQUJrs1","executionInfo":{"status":"aborted","timestamp":1763766667270,"user_tz":420,"elapsed":412,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test # 2 also musk out group 3 and fill by gpt2\n","\n","ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\", split=\"validation[:3]\")\n","\n","# Example policy: G1 → ε=0 (drop+fill), G4 → ε=∞ (keep), G2/G3 → finite (pass-through)\n","eps_map = {1: 0, 2: 0, 3: 0, 4: float(\"inf\")}\n","\n","total = Counter()\n","\n","for ex in ds:\n","    ctx, q = ex[\"context\"], ex[\"question\"]\n","\n","    # 1) Partition (for reporting and to align groups with BASE tokenizer)\n","    tok_ids, _, groups = partition_tokens_2x2(\n","        context=ctx,\n","        question=q,\n","        tokenizer=tokenizer,              # BASE tokenizer (same one used by your embeddings)\n","        embedding_table=embedding_table,\n","        tau=0.55,\n","    )\n","    counts = Counter(groups); total.update(groups)\n","\n","    # 2) Apply drop+GPT-2 fill (IDs variant; returns text + ids)\n","    # some copied form SD code...\n","\n","    privatized_ctx, _ = apply_drop_zero_keep_inf_gpt2fill_ids(\n","        token_ids=tok_ids,\n","        groups=groups,\n","        eps_dir_by_group=eps_map,\n","        tokenizer=tokenizer,              # BASE tokenizer\n","        gpt2_tok=gpt2_tok,                # GPT-2 tokenizer\n","        gpt2_model=gpt2_model,            # GPT-2 model\n","        deterministic=True,               # set False for sampling\n","        temperature=0.2,\n","        top_p=0.9, # or top k=50\n","        context_window=256,\n","    )\n","\n","    # Apply repair_coherent_gpt4\n","    repaired_ctx = repair_coherent_gpt4(privatized_ctx, answer_span=None)\n","\n","\n","    print(\"Q:\", q)\n","    print(\"Privatized context (Polar):\", privatized_ctx)\n","    print(\"Repaired context:\", repaired_ctx)"],"metadata":{"id":"u6cQrkkNJryz","executionInfo":{"status":"aborted","timestamp":1763766667274,"user_tz":420,"elapsed":415,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"U9pTwDctJr1E","executionInfo":{"status":"aborted","timestamp":1763766667275,"user_tz":420,"elapsed":415,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QmgX_UtcBE21","executionInfo":{"status":"aborted","timestamp":1763766667276,"user_tz":420,"elapsed":416,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test # 1 only musk out group 1&2 and fill by gpt4\n","\n","eps_map = {1: 0.0, 2: 0.0, 3: float(\"inf\"), 4: float(\"inf\")}\n","\n","ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\", split=\"validation[:3]\")\n","total = Counter()\n","\n","for ex in ds:\n","    ctx, q = ex[\"context\"], ex[\"question\"]\n","\n","    tok_ids, _, groups = partition_tokens_2x2(\n","        context=ctx,\n","        question=q,\n","        tokenizer=tokenizer,           # BASE tokenizer\n","        embedding_table=embedding_table,\n","        tau=0.55,\n","    )\n","    counts = Counter(groups); total.update(groups)\n","\n","    privatized_ctx, _ = apply_drop_zero_keep_inf_gpt4fill_ids(\n","        token_ids=tok_ids,\n","        groups=groups,\n","        eps_dir_by_group=eps_map,\n","        tokenizer=tokenizer,\n","        openai_client=client,\n","        model=\"gpt-4o-mini\",\n","        temperature=0.2,\n","        top_p=0.9,\n","        context_window=256,\n","        enforce_one_token=True,        # keep exactly 1 base token per drop\n","        # mask_groups={1,3},           # optional override independent of eps_map\n","    )\n","\n","    # Apply repair_coherent_gpt4\n","    repaired_ctx = repair_coherent_gpt4(privatized_ctx, answer_span=None)\n","\n","\n","    print(\"Q:\", q)\n","    print(\"Privatized context (Polar):\", privatized_ctx)\n","    print(\"Repaired context:\", repaired_ctx)"],"metadata":{"id":"x8TXupNtQG3U","executionInfo":{"status":"aborted","timestamp":1763766667296,"user_tz":420,"elapsed":435,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test # 2 also musk out group 3 and fill by gpt4\n","\n","eps_map = {1: 0.0, 2: 0.0, 3: 0.0, 4: float(\"inf\")}\n","\n","ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\", split=\"validation[:3]\")\n","total = Counter()\n","\n","for ex in ds:\n","    ctx, q = ex[\"context\"], ex[\"question\"]\n","\n","    tok_ids, _, groups = partition_tokens_2x2(\n","        context=ctx,\n","        question=q,\n","        tokenizer=tokenizer,           # BASE tokenizer\n","        embedding_table=embedding_table,\n","        tau=0.55,\n","    )\n","    counts = Counter(groups); total.update(groups)\n","\n","    privatized_ctx, _ = apply_drop_zero_keep_inf_gpt4fill_ids(\n","        token_ids=tok_ids,\n","        groups=groups,\n","        eps_dir_by_group=eps_map,\n","        tokenizer=tokenizer,\n","        openai_client=client,\n","        model=\"gpt-4o-mini\",\n","        temperature=0.2,\n","        top_p=0.9,\n","        context_window=256,\n","        enforce_one_token=True,        # keep exactly 1 base token per drop\n","        # mask_groups={1,3},           # optional override independent of eps_map\n","    )\n","\n","    # Apply repair_coherent_gpt4\n","    repaired_ctx = repair_coherent_gpt4(privatized_ctx, answer_span=None)\n","\n","\n","    print(\"Q:\", q)\n","    print(\"Privatized context (Polar):\", privatized_ctx)\n","    print(\"Repaired context:\", repaired_ctx)"],"metadata":{"id":"URg6YIOxQOT6","executionInfo":{"status":"aborted","timestamp":1763766667297,"user_tz":420,"elapsed":435,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cyGH4JGaSlbS","executionInfo":{"status":"aborted","timestamp":1763766667298,"user_tz":420,"elapsed":435,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test # 2 also musk out group 3 and fill by gpt4\n","\n","eps_map = {1: 0.0, 2: 0.0, 3: 0.0, 4: float(\"inf\")}\n","\n","ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\", split=\"validation\") # Use full validation set\n","total = Counter()\n","results_gpt4_mask_fill = [] # List to store results\n","\n","for ex in ds:\n","    ctx, q = ex[\"context\"], ex[\"question\"]\n","    answers = ex[\"answers\"][\"text\"] # Extract answers\n","\n","    tok_ids, _, groups = partition_tokens_2x2(\n","        context=ctx,\n","        question=q,\n","        tokenizer=tokenizer,           # BASE tokenizer\n","        embedding_table=embedding_table,\n","        tau=0.55,\n","    )\n","    counts = Counter(groups); total.update(groups)\n","\n","    privatized_ctx, _ = apply_drop_zero_keep_inf_gpt4fill_ids(\n","        token_ids=tok_ids,\n","        groups=groups,\n","        eps_dir_by_group=eps_map,\n","        tokenizer=tokenizer,\n","        openai_client=client,\n","        model=\"gpt-4o-mini\",\n","        temperature=0.2,\n","        top_p=0.9,\n","        context_window=256,\n","        enforce_one_token=True,        # keep exactly 1 base token per drop\n","        # mask_groups={1,3},           # optional override independent of eps_map\n","    )\n","\n","    # Apply repair_coherent_gpt4\n","    repaired_ctx = repair_coherent_gpt4(privatized_ctx, answer_span=None)\n","\n","    results_gpt4_mask_fill.append({\n","        \"question\": q,\n","        \"answers\": answers,\n","        \"original_context\": ctx,\n","        \"privatized_context\": privatized_ctx,\n","        \"repaired_context\": repaired_ctx\n","    })\n","\n","    print(\"Q:\", q)\n","    print(\"Privatized context (GPT-4 Mask Fill):\", privatized_ctx)\n","    print(\"Repaired context:\", repaired_ctx)\n","\n","# Create a pandas DataFrame and save it to CSV\n","df_results_gpt4_mask_fill = pd.DataFrame(results_gpt4_mask_fill)\n","df_results_gpt4_mask_fill.to_csv(\"squad_gpt4_mask_fill.csv\", index=False)\n","\n","print(\"\\nProcessing complete. Results saved to squad_gpt4_mask_fill.csv\")"],"metadata":{"id":"4ZBjkQzLSldj","executionInfo":{"status":"aborted","timestamp":1763766667298,"user_tz":420,"elapsed":434,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GmtVPio0S6P3","executionInfo":{"status":"aborted","timestamp":1763766667300,"user_tz":420,"elapsed":436,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a2963b0f","executionInfo":{"status":"aborted","timestamp":1763766667301,"user_tz":420,"elapsed":436,"user":{"displayName":"Feng Wei Tian","userId":"15545888329392400707"}}},"source":["import time\n","import torch\n","\n","# Initialize timing variables\n","total_polar_time = 0\n","polar_perturb_count = 0\n","total_laplace_time = 0\n","laplace_perturb_count = 0\n","\n","# Define epsilon map\n","eps_map = {1: 200, 2: 100, 3: 500, 4: 400}\n","\n","# Select first 5 examples from the validation set\n","# Assuming 'ds' is already loaded. If not, load it: ds = load_dataset(\"Setpember/Fantasy-SQUAD_10\")\n","validation_subset = ds['validation'].select(range(5))\n","\n","print(\"Starting timing comparison on 5 examples...\")\n","\n","for i, ex in enumerate(validation_subset):\n","    ctx = ex[\"context\"]\n","    q = ex[\"question\"]\n","\n","    # Partition tokens\n","    tok_ids, _, groups = partition_tokens_2x2(\n","        context=ctx,\n","        question=q,\n","        tokenizer=tokenizer,\n","        embedding_table=embedding_table,\n","        tau=0.50\n","    )\n","\n","    if tok_ids:\n","        # Measure Polar time\n","        start_time = time.time()\n","        _ = apply_stamp(\n","            torch.tensor(tok_ids, device=embedding_table.device),\n","            groups,\n","            embedding_table,\n","            tokenizer,\n","            eps_dir_by_group=eps_map,\n","        )\n","        end_time = time.time()\n","        total_polar_time += (end_time - start_time)\n","        polar_perturb_count += 1\n","\n","        # Measure Laplace time\n","        start_time = time.time()\n","        _ = apply_stamp_laplace(\n","            torch.tensor(tok_ids, device=embedding_table.device),\n","            groups,\n","            embedding_table,\n","            tokenizer,\n","            eps_dir_by_group=eps_map,\n","        )\n","        end_time = time.time()\n","        total_laplace_time += (end_time - start_time)\n","        laplace_perturb_count += 1\n","\n","# Calculate averages\n","avg_polar = total_polar_time / polar_perturb_count if polar_perturb_count > 0 else 0\n","avg_laplace = total_laplace_time / laplace_perturb_count if laplace_perturb_count > 0 else 0\n","\n","print(f\"\\nProcessed {polar_perturb_count} examples.\")\n","print(f\"Average Polar Time: {avg_polar:.6f} s\")\n","print(f\"Average Laplace Time: {avg_laplace:.6f} s\")"],"execution_count":null,"outputs":[]}]}